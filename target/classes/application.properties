# Server Configuration
server.port=8080

# Ollama Configuration
# This assumes Ollama is running locally on default port 11434
spring.ai.ollama.base-url=http://localhost:11434

# Chat Model Configuration (for generating answers)
spring.ai.ollama.chat.model=llama3.2
spring.ai.ollama.chat.options.temperature=0.2
spring.ai.ollama.chat.options.top-p=0.9

# Embedding Model Configuration (for creating vector embeddings)
# nomic-embed-text produces 768-dimensional embeddings automatically
spring.ai.ollama.embedding.model=nomic-embed-text

# Vector Store Configuration
vector.store.file-path=./data/vector-store.json
documents.path=./data/tax-documents

# Logging
logging.level.org.springframework.ai=DEBUG
logging.level.com.example=DEBUG